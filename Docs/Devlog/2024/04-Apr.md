# April 2024

## April 21 - Progress - Lexer, Parser, ARM and Mach-O
I've made significant progress on the implementation. We have an incremental lexer, parser and now, with native code generation and linking on macOS ARM. I've built many many different iterations of the lexer and parser in the past, exploring every major approach from recursive descent, to shunting yard, to pratt parsers, to table driven parsers. This version of the parser has characteristics from all of them, while offering the best balance of performance and flexibility. It's hard to classify it as one thing, but overall it's a single pass state-machine like approach, with each state represented in code as explicit tail-recursive (direct-threaded) functions (giving us hooks to handle error cases really well). We use bitset based matching, and store additional metadata in the AST to eliminate recursive-descent like backtracking. The AST format in general was one of the big improvements this generation - the compact, 64 bit based AST pose challenges to carry all of the information across and link everything up, but it's been completely worth it. Structuring things as incremental, queue-based chunks also lets me test each layer independently, while giving me the freedom to glue them together in flexible ways.

What I have much less experience with is building an assembler and a linker. This was my first foray into grappling with these encodings at the bit level. Operating at this level gives you a lot more appreciation for details that are normally abstracted away. ARM has gotten a lot more CISC like - in both the variety of instructions, and in how many things you can do in a single instruction (operation + indirect loads / store / masking + shift). Navigating these multiple layers of bit-patterns to figure out the operation stumped me on day 1, until I figured out I was doing little-endian wrong. Loading arbitrary immediates is also fraught with special cases. I've only dealt with a handful of instructions so far, but I'm looking forward to learning the full instruction set in detail.

I made the decision to build an embedded linker. It avoids an extra layer of indirection, and offers control over a lot more details. We already have the assembly code, how hard can it be to make it executable? Right? Very hard, it turns out. I could piece together the Mach-O format by reverse engineering what the system loader assembles. With the help of a hex editor, diffing xxd output, objtools and some visual Mach-O explorers, I could piece together the binary format. But the biggest hurdle thus far has been working around Apple's System Integrity Protection features. That completely disallows the kind of static linking I want to do, if your binary lacks certain undocumented flags or dynamic linking sections, it just won't run. And all binaries have to be signed with a code signature, which requires a system certificate. Except there's an obscure mechanism for ad-hoc signing, which is what Go, Zig and Clang use. But for some reason, the signature code-directory is big-endian unlike the rest of the entire format. It has been a pain, but I've got it working - there's still more work remaining, but this is a good start. 

There's two potential paths I can take from here. Right now my lexer and parser only deals with the most basic binary expressions. It has no statements, no variables, no control structures, etc. It has the framework and hooks for it, but not the full implementation. I can either start building those features out and piping them through the layers, or I can start building the additional remaining layers I'll eventually need. In general, getting things working end-to-end, but inedependently, has served me well. So rather than adding more surface area of features first, I want to get the plumbing and architecture right while things are still small. That gives me the freedom to shift things around as needed rather than discovering some unknown much further down. The two hardest, most ambiguous problems we have currently is the dependent type system and the super optimizer. The type system relies on a few more layers and language capabilities not yet built out. So I'm going to tackle the super optimizer next.

# Spaces in variable names
Besides implementation, there's a major syntactic change I'm seriously considering. Spaces in variable names. It generally considered more trouble than it's worth, but I think the costs are overstated and the benefits to readability is significant. 

There's a handful of restrictions it places on parsing:
First, the lexer has to be able to identify variables with spaces as a single variable. You can't combine it with concepts where two identifiers appearing next to each other is meaningful. Previously, we used that as the product type - Even Int. Haskell uses that as a form of function composition, and some other languages use it for function applications. In our case, we can still support product types because the lexer can recognize Types as always starting with a capital letter - so compound types can be supported alongside without ambiguity.
The second restriction it imposes is on keywords and infix operators. Prefix keywords like "if" and "for" are not an issue - we can easily enforce that variable names shouldn't start with those. The problematic keywords are short, useful operators which appear within an expression. Things like "x in list" or "a is not b". I love that style of python operators, but it doesn't play well with spaced variables. You'll have to restrict that variable names cannot contain those keywords, which is a much more significant restriction you're likely to run into more often. The solution is two fold. Use symbolic operators - &&, ||, etc. and rely on methods instead of infix operators. Symbolic operators provide visual distinction, which help break up large, complex expressions, helping you visually tokenize an expression. 
`some variable and something else` vs `some variable && something else`.
The tradeoff with infix operators requires some serious consideration. On the one hand, methods give more uniform structure to the language. Java does this well and so does smalltalk. But there are certain constructs which are more naturally expressed as operators rather than methods - `elem in list` focuses on the element you're currently indexed into, while `list.contains(elem)` has the containing list as the subject. Maintaining the same subject makes the code clearer, with less mental context switching. There are several possible solutions to representing infix operators in the presence of spaces. 
The key problem is differentiating operators from identifiers. You can do that with special tokens like perl, `x $in y`, or by surrounding it in appostrophe or backticks like haskell (which also has the distinct advantage of supporting operators with spaces as well) `x 'in' y` - appostrophe is easier to type, but IDE autocomplete can actually get in the way here, and backticks are a bit too hard to type for something so common. A good balanced approach may be to use capitalization - `x IN y`, which gives it the same visual distinction without compromising readability.

The idea has grown on me after trying it out on paper first. The readability improvements are significant - it leans on the pattern recognition of words and reading of normal sentences, like we do in all other contexts. That frees some mental tax spent on splitting apart combinedSequencesOfWords and reads more naturally. The readability benefits matter the most, but it's also significantly easier to type - the space is the biggest key on the keyboard after all, and uses our strongest fingers. It avoids the RSI inducing, pinky-strain of shift + _ on long coding sessions. 

There are some stylistic changes we have to adopt for operators like '.'. If you do `some long variable.some property` it groups it in a weird way. It feels like variable and some are connected, but it breaks the cohesion of the structure. The alternative options are `some long variable . some property` or `some long variable. some property` or `some long variable .some property`. The first is overly aggressive with spaces, leading to longer lines, but it gives the '.' some visual weight - but maybe a bit too much weight given how often it can appear in `deeply . nested . property . accesses . with many . layers`. Using the period style accessor also leans on the way we're taught to read such expressions as a mental 'pause' or 'separator'. `deeply. nested. property. accesses. with many. layers`. The . on the symbol feels a bit like Smalltalk's message syntax. It's also a good option.

This isn't the first language to do this. Fortran and Cobol in the past also supported spaces, though their implementation differs. It requires careful language design to avoid a situation where the language supports it but general recommendation is to avoid it. You have to design around it from the start and root out all ambiguities.